{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasguha/Shreyas_AI/blob/main/numpy_digit_classifier/MNIST_digitclassifier_2layer_10node.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2XbV4jIvlXk"
      },
      "source": [
        "# Simple MNIST NN using numpy (2 layers, 10 neurons)\n",
        "\n",
        "Here I implement a simple neural network with 1 input layer, 2 hidden layers, and 1 output layer for classifying the MNIST handwritten digit recognition database. Both the hidden layers contain 10 neurons, just like the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPUe9RcSvlXn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wv5obJSlIw5O"
      },
      "outputs": [],
      "source": [
        "pd_data = pd.read_csv('/content/drive/MyDrive/AI/Public Dataset/MNIST digit database/mnist_train.csv')\n",
        "#pd_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bdT2AxaeHSaV"
      },
      "outputs": [],
      "source": [
        "data = np.array(pd_data)\n",
        "np.random.shuffle(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp3xWg3wHmA8"
      },
      "outputs": [],
      "source": [
        "# you should see 7, 3, 8.........0, 0, 4\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XKChITvBvlXo"
      },
      "outputs": [],
      "source": [
        "m, n = data.shape\n",
        "\n",
        "data_dev = data[0:1000].T\n",
        "# Y_dev has 1 row, and columns from 1 to 1000 each column containing the actual label of that datapoint\n",
        "Y_dev = data_dev[0]\n",
        "# X_dev has 785 rows, each corresponding to different pixel values, and 1000 columns, each for a different datapoint\n",
        "X_dev = data_dev[1:n]\n",
        "X_dev = X_dev / 255.\n",
        "\n",
        "data_train = data[1000:m].T\n",
        "# Y_train has 1 row, and columns from 1000 to 60000 each column containing the actual label of that datapoint\n",
        "Y_train = data_train[0]\n",
        "# X_train has 785 rows, each corresponding to different pixel values, and 59000 columns, each for a different datapoint\n",
        "X_train = data_train[1:n]\n",
        "X_train = X_train / 255.\n",
        "#_,m_train = X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqNFHSLYvlXp",
        "outputId": "f0d06ccd-6ebd-46ac-bd61-bd4c44d25f9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 59000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDX-KxtfHtvY"
      },
      "source": [
        "Our NN will have a simple two-layer architecture. Input layer $a^{[0]}$ will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer $a^{[1]}$ will have 10 units with ReLU activation, and finally our output layer $a^{[2]}$ will have 10 units corresponding to the ten digit classes with softmax activation.\n",
        "\n",
        "**Forward propagation**\n",
        "\n",
        "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
        "$$A^{[1]} = f(Z^{[1]}))$$\n",
        "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
        "$$A^{[2]} = g(Z^{[2]})$$\n",
        "$$Z^{[3]} = W^{[3]} A^{[2]} + b^{[3]}$$\n",
        "$$A^{[3]} = softmax(Z^{[3]})$$\n",
        "\n",
        "**Backward propagation**\n",
        "\n",
        "$$dZ^{[3]} = A^{[3]} - Y$$\n",
        "$$dW^{[3]} = \\frac{1}{m} dZ^{[3]} A^{[2]T}$$\n",
        "$$db^{[3]} = \\frac{1}{m} \\Sigma {dZ^{[3]}}$$\n",
        "$$dZ^{[2]} = W^{[3]T} dZ^{[3]} .* f^{[2]\\prime} (z^{[2]})$$\n",
        "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
        "$$db^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n",
        "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n",
        "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^{T}$$\n",
        "$$db^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$\n",
        "\n",
        "**Parameter updates**\n",
        "\n",
        "$$W^{[3]} := W^{[3]} - \\alpha dW^{[3]}$$\n",
        "$$b^{[3]} := b^{[3]} - \\alpha db^{[3]}$$\n",
        "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
        "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
        "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
        "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
        "\n",
        "**Vars and shapes**\n",
        "\n",
        "Forward prop\n",
        "\n",
        "- $A^{[0]} = X$: 784 x 1\n",
        "- $Z^{[1]} \\sim A^{[1]}$: 10 x 1\n",
        "- $W^{[1]}$: 10 x 784 (as $W^{[1]} A^{[0]} \\sim Z^{[1]}$)\n",
        "- $B^{[1]}$: 10 x 1\n",
        "- $Z^{[2]} \\sim A^{[2]}$: 10 x 1\n",
        "- $W^{[2]}$: 10 x 10 (as $W^{[2]} A^{[1]} \\sim Z^{[2]}$)\n",
        "- $B^{[2]}$: 10 x 1\n",
        "- $W^{[3]}$: 10 x 10 (as $W^{[3]} A^{[2]} \\sim Z^{[3]}$)\n",
        "- $B^{[3]}$: 10 x 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q6IG-M_o6gHq"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "    W1 = np.random.rand(10, 784) - 0.5\n",
        "    b1 = np.random.rand(10, 1) - 0.5\n",
        "    W2 = np.random.rand(10, 10) - 0.5\n",
        "    b2 = np.random.rand(10, 1) - 0.5\n",
        "    W3 = np.random.rand(10, 10) - 0.5\n",
        "    b3 = np.random.rand(10, 1) - 0.5\n",
        "    return W1, b1, W2, b2, W3, b3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1su9T0vTvlXq"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "    W1 = np.random.rand(10, 784) - 0.5\n",
        "    b1 = np.random.rand(10, 1) - 0.5\n",
        "    W2 = np.random.rand(10, 10) - 0.5\n",
        "    b2 = np.random.rand(10, 1) - 0.5\n",
        "    W3 = np.random.rand(10, 10) - 0.5\n",
        "    b3 = np.random.rand(10, 1) - 0.5\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "def Sigmoid(Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def Sigmoid_deriv(Z):\n",
        "    return np.exp(-Z) / pow(1 + np.exp(-Z), 2)\n",
        "\n",
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z) / sum(np.exp(Z))\n",
        "    return A\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def one_hot(Y):\n",
        "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_Y = one_hot_Y.T\n",
        "    return one_hot_Y\n",
        "\n",
        "\n",
        "#------------------------------------------MAKE CHANGES HERE FOR DIFFERENT ACTIVATION FUNCTIONS IN DIFFERENT LAYERS---------------------------------\n",
        "def forward_prop(W1, b1, W2, b2, W3, b3, X):\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = Sigmoid(Z1)                      #here\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)                         #here\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    A3 = softmax(Z3)\n",
        "    return Z1, A1, Z2, A2, Z3, A3\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y):\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    dZ3 = A3 - one_hot_Y\n",
        "    dW3 = 1 / m * dZ3.dot(A2.T)\n",
        "    db3 = 1 / m * np.sum(dZ3)\n",
        "    dZ2 = W3.T.dot(dZ3) * Sigmoid_deriv(Z2)  #here\n",
        "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "    db2 = 1 / m * np.sum(dZ2)\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)  #here\n",
        "    dW1 = 1 / m * dZ1.dot(X.T)\n",
        "    db1 = 1 / m * np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2, dW3, db3\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1\n",
        "    W2 = W2 - alpha * dW2\n",
        "    b2 = b2 - alpha * db2\n",
        "    W3 = W3 - alpha * dW3\n",
        "    b3 = b3 - alpha * db3\n",
        "    return W1, b1, W2, b2, W3, b3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DrT65JJtvlXs"
      },
      "outputs": [],
      "source": [
        "def get_predictions(A3):\n",
        "    return np.argmax(A3, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent_start(X, Y, alpha, iterations):\n",
        "    W1, b1, W2, b2, W3, b3 = init_params()\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
        "        dW1, db1, dW2, db2, dW3, db3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y)\n",
        "        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n",
        "        if i % 50 == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            predictions = get_predictions(A3)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "def gradient_descent_modify(W1, b1, W2, b2, W3, b3, X, Y, alpha, iterations):\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
        "        dW1, db1, dW2, db2, dW3, db3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y)\n",
        "        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n",
        "        if i % 50 == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            predictions = get_predictions(A3)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "\n",
        "def make_predictions(X, W1, b1, W2, b2, W3, b3):\n",
        "    _, _, _, _, _, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
        "    predictions = get_predictions(A3)\n",
        "    return predictions\n",
        "\n",
        "def test_prediction(index, W1, b1, W2, b2, W3, b3):\n",
        "    current_image = X_train[:, index, None]\n",
        "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2, W3, b3)\n",
        "    label = Y_train[index]\n",
        "    print(\"Prediction: \", prediction)\n",
        "    print(\"Label: \", label)\n",
        "\n",
        "    current_image = current_image.reshape((28, 28)) * 255\n",
        "    plt.gray()\n",
        "    plt.imshow(current_image, interpolation='nearest')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syx5A81kvlXs"
      },
      "outputs": [],
      "source": [
        "#W1, b1, W2, b2, W3, b3 = gradient_descent_start(X_train, Y_train, 0.10, 100)\n",
        "#W1, b1, W2, b2, W3, b3 = init_params()\n",
        "W1, b1, W2, b2, W3, b3 = gradient_descent_modify(W1, b1, W2, b2, W3, b3, X_train, Y_train, 0.1, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsQqQiIHvlXv"
      },
      "outputs": [],
      "source": [
        "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2, W3, b3)\n",
        "get_accuracy(dev_predictions, Y_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqZDsz6DKfB3"
      },
      "source": [
        "ReLU + ReLU\n",
        "- On 500 timesteps we get 82.3% accuracy on our training data and 83.5% accuracy on our test data\n",
        "- 500 more timesteps, train:87.4% test:87.6%\n",
        "- 340 more timesteps to reach train:88.7% test:88.1%\n",
        "- 1500 more timesteps, train:91.1% test:90%\n",
        "- 2000 more timesteps, train:92.3% test:91.8%\n",
        "-- at this point, at learning rate of 1, the accuracy seems to keep fluctuating around that 92.5% value, thus we reduce the learning rate to 0.5 and train further\n",
        "- 2000 more timesteps, train:93.1% test:92.5%\n",
        "- 2000 more timesteps, train:93.3% test:92.3%\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 861823,
          "sourceId": 3004,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30035,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
