{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3004,
          "databundleVersionId": 861823,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30035,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasguha/Shreyas_AI/blob/main/numpy_digit_classifier/MNIST_digitclassifier_1layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple MNIST NN using numpy\n",
        "\n",
        "Here I implement a simple neural network with 1 input layer, 1 hidden layer, and 1 output layer for classifying the MNIST handwritten digit recognition database. The hidden layer consists of 10 neurons, just like the output layer"
      ],
      "metadata": {
        "id": "S2XbV4jIvlXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "jPUe9RcSvlXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the data**"
      ],
      "metadata": {
        "id": "LK4HxwRUKloL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_data = pd.read_csv('/content/drive/MyDrive/AI/Public Dataset/MNIST digit database/mnist_train.csv')\n",
        "data = np.array(pd_data)\n",
        "np.random.shuffle(data)"
      ],
      "metadata": {
        "id": "bdT2AxaeHSaV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dividing the data into training and dev**"
      ],
      "metadata": {
        "id": "5Qi2LPGQKrkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = data.shape\n",
        "\n",
        "data_dev = data[0:1000].T\n",
        "# Y_dev has 1 row, and columns from 1 to 1000 each column containing the actual label of that datapoint\n",
        "Y_dev = data_dev[0]\n",
        "# X_dev has 785 rows, each corresponding to different pixel values, and 1000 columns, each for a different datapoint\n",
        "X_dev = data_dev[1:n]\n",
        "X_dev = X_dev / 255.\n",
        "\n",
        "data_train = data[1000:m].T\n",
        "# Y_train has 1 row, and columns from 1000 to 60000 each column containing the actual label of that datapoint\n",
        "Y_train = data_train[0]\n",
        "# X_train has 785 rows, each corresponding to different pixel values, and 59000 columns, each for a different datapoint\n",
        "X_train = data_train[1:n]\n",
        "X_train = X_train / 255.\n",
        "#_,m_train = X_train.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "XKChITvBvlXo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "DqNFHSLYvlXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111cfb4c-83bd-47cc-a6ad-cef4ee8b12ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 59000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward propagation**\n",
        "\n",
        "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
        "$$A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))$$\n",
        "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
        "$$A^{[2]} = softmax(Z^{[2]})$$\n",
        "\n",
        "**Backward propagation**\n",
        "\n",
        "$$dZ^{[2]} = A^{[2]} - Y$$\n",
        "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
        "$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n",
        "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n",
        "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n",
        "$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$\n",
        "\n",
        "**Parameter updates**\n",
        "\n",
        "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
        "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
        "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
        "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
        "\n",
        "**Vars and shapes**\n",
        "\n",
        "Forward prop\n",
        "\n",
        "- $A^{[0]} = X$: 784 x 1\n",
        "- $Z^{[1]} \\sim A^{[1]}$: 10 x 1\n",
        "- $W^{[1]}$: 10 x 784 (as $W^{[1]} A^{[0]} \\sim Z^{[1]}$)\n",
        "- $B^{[1]}$: 10 x 1\n",
        "- $Z^{[2]} \\sim A^{[2]}$: 10 x 1\n",
        "- $W^{[2]}$: 10 x 10 (as $W^{[2]} A^{[1]} \\sim Z^{[2]}$)\n",
        "- $B^{[2]}$: 10 x 1\n",
        "\n"
      ],
      "metadata": {
        "id": "8SDZ55TRvlXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define usefult functions such as parameter initialization, ReLU, softmax, forward propogation, derivative of ReLU, one hot encoding of Y, backward propogation and parameter update**"
      ],
      "metadata": {
        "id": "IA9BUFJgLHA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params():\n",
        "    W1 = np.random.rand(10, 784) - 0.5\n",
        "    b1 = np.random.rand(10, 1) - 0.5\n",
        "    W2 = np.random.rand(10, 10) - 0.5\n",
        "    b2 = np.random.rand(10, 1) - 0.5\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z) / sum(np.exp(Z))\n",
        "    return A\n",
        "\n",
        "def forward_prop(W1, b1, W2, b2, X):\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def one_hot(Y):\n",
        "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_Y = one_hot_Y.T\n",
        "    return one_hot_Y\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    dZ2 = A2 - one_hot_Y\n",
        "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "    db2 = 1 / m * np.sum(dZ2)\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = 1 / m * dZ1.dot(X.T)\n",
        "    db1 = 1 / m * np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1\n",
        "    W2 = W2 - alpha * dW2\n",
        "    b2 = b2 - alpha * db2\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "trusted": true,
        "id": "1su9T0vTvlXq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define some more important functions such as get predictions (to convert output of model A2 to something that can be compared with the label data), accuracy calculator, make_predictions and test prediction (which can pass a specific input through the model and show us the output), and 2 functions for gradient descent, namely gradient_descent_start and gradient_descent_modify which can be used to start the process of training the model and further finetune the model respectively**"
      ],
      "metadata": {
        "id": "5JshyD8TLYdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent_start(X, Y, alpha, iterations):\n",
        "    W1, b1, W2, b2 = init_params()\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "        if i % 10 == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            predictions = get_predictions(A2)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def gradient_descent_modify(W1, b1, W2, b2, X, Y, alpha, iterations):\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "        if i % 10 == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            predictions = get_predictions(A2)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "def make_predictions(X, W1, b1, W2, b2):\n",
        "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "    predictions = get_predictions(A2)\n",
        "    return predictions\n",
        "\n",
        "def test_prediction(index, W1, b1, W2, b2):\n",
        "    current_image = X_train[:, index, None]\n",
        "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
        "    label = Y_train[index]\n",
        "    print(\"Prediction: \", prediction)\n",
        "    print(\"Label: \", label)\n",
        "\n",
        "    current_image = current_image.reshape((28, 28)) * 255\n",
        "    plt.gray()\n",
        "    plt.imshow(current_image, interpolation='nearest')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "DrT65JJtvlXs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1, W2, b2 = gradient_descent_start(X_train, Y_train, 0.10, 500)"
      ],
      "metadata": {
        "trusted": true,
        "id": "syx5A81kvlXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
        "get_accuracy(dev_predictions, Y_dev)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DsQqQiIHvlXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- On 500 timesteps we get 83.1% accuracy on our training data and 82.5% accuracy on our test data\n",
        "- 500 more timesteps, train:87.4% test:87.3%\n",
        "- 340 more timesteps to reach train:90% test:89.7%\n",
        "- 1500 more timesteps, train:92.2% test:91%\n",
        "- 2000 more timesteps, train:93.3% test:92.3%\n",
        "- 4000 more timesteps, train:93.9% test:92.4%"
      ],
      "metadata": {
        "id": "IqZDsz6DKfB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore we conclude that the accuracy of this single layer neural network peaks at about 92.4% on test data\n"
      ],
      "metadata": {
        "id": "HaGcWzuwkGwh"
      }
    }
  ]
}
